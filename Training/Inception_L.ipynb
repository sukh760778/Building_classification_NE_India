{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = [round(i*0.0001,4) for i in range(1,8)]\n",
    "print(f'Coarse Learning rates will be chosen from : {LR}')\n",
    "lr_tune = 1e-5\n",
    "SEED = 123\n",
    "Save = True\n",
    "batch_size = 10\n",
    "img_shape = (832,535)\n",
    "epochs = epochs_train = 30\n",
    "epochs_tune = 2\n",
    "val_split  = 0.3 # Training vs. Validation split\n",
    "class_names = ['Assam_Type','Metal_Sheet','RCC','Vacant']\n",
    "\n",
    "work_foldr = r'/home/user/Desktop/Backup_E/phd/CNN_Building_classification'\n",
    "\n",
    "Loss='sparse_categorical_crossentropy' # Loss type \n",
    "\n",
    "data_dir = data_dir_train = f'{work_foldr}/Augmented_30000' # Directory of images   \n",
    "\n",
    "dir_txt = f'{work_foldr}/Hyper_p_R_E.json' # Latest Hyper Params are stored here\n",
    "\n",
    "past = f'{work_foldr}'# All previous best are stored here\n",
    "dir_model1 = f'{work_foldr}/model_E.keras'\n",
    "dir_model = f'{work_foldr}/model_E.weights.h5'# The model's weights are stored here ## Use in conjunction with .save_weights() and .load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def Init_model():\n",
    "    \"\"\"\n",
    "    This function initializes the custom model and returns it\n",
    "    Returns: Model (Sequential)\n",
    "    \"\"\"\n",
    "    global img_shape\n",
    "    global class_names\n",
    "    classes = len(class_names)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    pretrained_model = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(img_shape[0],img_shape[1],3),\n",
    "        pooling='avg',\n",
    "        classes=classes,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "    \n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(pretrained_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_aug(data_dir, test_split=0.1):\n",
    "    \"\"\"\n",
    "    This function takes the directory where the image dataset is stored and the optional test split (default= 10%)\n",
    "    Returns: three values that contain segregated images in the order of (training, validation, testing) datasets\n",
    "    \"\"\"\n",
    "    global SEED, batch_size, val_split, img_shape, class_names\n",
    "    \n",
    "    # Calculate the actual validation split considering the test split\n",
    "    actual_val_split = val_split / (1 - test_split)\n",
    "    \n",
    "    # Load the full dataset\n",
    "    full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        seed=SEED,\n",
    "        image_size=img_shape,\n",
    "        batch_size=None,  # Load without batching first\n",
    "        class_names=class_names,\n",
    "    )\n",
    "    \n",
    "    # Print dataset size\n",
    "    print(f\"Total number of samples: {tf.data.experimental.cardinality(full_ds).numpy()}\")\n",
    "    \n",
    "    # Split the dataset into train+val and test\n",
    "    dataset_size = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "    train_val_size = int((1 - test_split) * dataset_size)\n",
    "    test_size = dataset_size - train_val_size\n",
    "    \n",
    "    train_val_ds = full_ds.take(train_val_size)\n",
    "    test_ds = full_ds.skip(train_val_size)\n",
    "    \n",
    "    # Split train+val into train and validation\n",
    "    train_size = int((1 - actual_val_split) * train_val_size)\n",
    "    val_size = train_val_size - train_size\n",
    "    \n",
    "    train_ds = train_val_ds.take(train_size)\n",
    "    val_ds = train_val_ds.skip(train_size)\n",
    "    \n",
    "    # Apply batching\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    val_ds = val_ds.batch(batch_size)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    \n",
    "    print(f\"Train samples: {train_size}\")\n",
    "    print(f\"Validation samples: {val_size}\")\n",
    "    print(f\"Test samples: {test_size}\")\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(test_ds):\n",
    "    \"\"\"\n",
    "    Turns batches to a single array\n",
    "    Returns: images,labels {List,List} \n",
    "    \"\"\"\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for images, labels in test_ds:\n",
    "        test_images.append(images.numpy())\n",
    "        test_labels.append(labels.numpy())\n",
    "    \n",
    "    test_images = np.concatenate(test_images, axis=0)\n",
    "    test_labels = np.concatenate(test_labels, axis=0)\n",
    "    \n",
    "    return test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary that contains all hyperparameters\n",
    "hyper_params = {\n",
    "    'lr': 0.0001 ,\n",
    "    'batch_size': batch_size ,\n",
    "    'img_shape': img_shape, #(832,535)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for accuracy and other scores\n",
    "train_ds, val_ds, test_ds = img_aug(data_dir)\n",
    "test_images, test_labels = prepare_test_data(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is loaded and compiled with the various parameters\n",
    "\n",
    "model = Init_model()\n",
    "train_ds, val_ds, test_ds = img_aug(data_dir_train)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate = hyper_params['lr']),\n",
    "    loss = Loss,\n",
    "    metrics=['accuracy'],\n",
    "    \n",
    ")\n",
    "print('Model Setup done successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories keeps all the models trained in this instannce. Can be used for comparative analysis\n",
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Early stopping, Learning rate reduction on plateau and saving of best model (callback.ModelCheckpoint) declared and instantiated here\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n",
    "model_chk_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = dir_model,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradual unfreezing of layers and subtle training\n",
    "\n",
    "for j in range(-1,-10,-1):\n",
    "    model.get_layer(\"inception_v3\").layers[j].trainable = True\n",
    "    if(j<-1):\n",
    "        model.get_layer(\"inception_v3\").layers[j+1].trainable = False\n",
    "    print(f'layer {j} unfrozen')\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs_train,  \n",
    "        callbacks=[early_stopping, lr_scheduler,model_chk_callback]\n",
    "      )\n",
    "    histories.append(history)\n",
    "print('Model Trained Succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def display_images_with_labels(dataset, class_names, num_images=9):\n",
    "    \"\"\"\n",
    "    Display images with labels for a peek at the dataset\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(min(num_images, len(images))):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            \n",
    "            label = labels[i].numpy()\n",
    "            class_name = class_names[label]\n",
    "            \n",
    "            plt.title(f\"Class: {class_name}\\nLabel: {label}\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images_with_labels(train_ds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(model, test_images, test_labels):\n",
    "    \"\"\"\n",
    "    Used to find accuracy, F1 score and Matthew's correlation Coefficient of the model given as input\n",
    "    Returns: Accuracy, f1-score, mathhew's correlation coefficient in this order \n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Convert one-hot encoded labels to class indices if necessary\n",
    "    if len(test_labels.shape) > 1:\n",
    "        true_classes = np.argmax(test_labels, axis=1)\n",
    "    else:\n",
    "        true_classes = test_labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    f1 = f1_score(true_classes, predicted_classes, average='macro')\n",
    "    mcc = matthews_corrcoef(true_classes, predicted_classes)\n",
    "    \n",
    "    return accuracy, f1, mcc\n",
    "\n",
    "accuracy, f1, mcc = calculate_metrics(model, test_images, test_labels)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_model2 = f'{work_foldr}/Inceptionv3_30000.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(dir_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "\n",
    "# This section displays the detailed metrics of the model\n",
    "\n",
    "\n",
    "def calculate_metrics(model, test_images, test_labels, class_names):\n",
    "    \"\"\"\n",
    "    A more detailed metrics calculator. \n",
    "    Returns: overall_accuracy, overall_f1, overall_mcc, per_class_metrics, true_classes, predicted_classes, confusion_matrix\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Convert one-hot encoded labels to class indices if necessary\n",
    "    if len(test_labels.shape) > 1:\n",
    "        true_classes = np.argmax(test_labels, axis=1)\n",
    "    else:\n",
    "        true_classes = test_labels\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "    overall_f1 = f1_score(true_classes, predicted_classes, average='macro')\n",
    "    overall_mcc = matthews_corrcoef(true_classes, predicted_classes)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    n_classes = cm.shape[0]\n",
    "    per_class_metrics = []\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        tn = cm.sum() - tp - fp - fn\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + fp + fn + tn) if (tp + fp + fn + tn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        mcc_numerator = (tp * tn - fp * fn)\n",
    "        mcc_denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_numerator / mcc_denominator if mcc_denominator != 0 else 0\n",
    "        \n",
    "        per_class_metrics.append({\n",
    "            'class': class_names[i],\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'mcc': mcc\n",
    "        })\n",
    "    \n",
    "    return overall_accuracy, overall_f1, overall_mcc, per_class_metrics, true_classes, predicted_classes, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Plotter of confusion matrix\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "#Reasserting class_names for ease of access\n",
    "class_names = ['Assam_Type', 'Metal_Sheet', 'RCC', 'Vacant']\n",
    "\n",
    "# Assuming `model`, `test_images`, and `test_labels` are declared and defined beforehand\n",
    "overall_accuracy, overall_f1, overall_mcc, per_class_metrics, true_classes, predicted_classes, cm = calculate_metrics(model, test_images, test_labels, class_names)\n",
    "\n",
    "# Print overall results\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
    "print(f\"Overall Matthews Correlation Coefficient: {overall_mcc:.4f}\")\n",
    "\n",
    "# Print per-class results\n",
    "print(\"\\nPer-class metrics:\")\n",
    "for metrics in per_class_metrics:\n",
    "    print(f\"Class {metrics['class']}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  MCC: {metrics['mcc']:.4f}\")\n",
    "\n",
    "# Call the function to plot the confusion matrix\n",
    "plot_confusion_matrix(cm, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Piece of code that plots the validation accuracy trend of all trained models\n",
    "\"\"\"\n",
    "fig1 = plt.gcf()\n",
    "accuracy = []\n",
    "val_accuracy = []\n",
    "for history in histories:\n",
    "    history = history.history\n",
    "    accuracy += history['accuracy']\n",
    "    val_accuracy += history['val_accuracy']\n",
    "plt.plot(accuracy)\n",
    "plt.plot(val_accuracy)\n",
    "plt.axis(ymin=0.2,ymax=1)\n",
    "plt.grid()\n",
    "plt.title(f'Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation']+[i for i in range(len(histories))])\n",
    "plt.show()\n",
    "    # i += 1\n",
    "# plt.legend(['train', 'validation']+[i for i in range(len(histories))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Piece of code that plots the validation loss trend of all trained models\n",
    "\"\"\"\n",
    "loss = []\n",
    "val_loss = []\n",
    "for history in histories:\n",
    "    history = history.history\n",
    "    loss += history['loss']\n",
    "    val_loss += history['val_loss']\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.grid()\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
